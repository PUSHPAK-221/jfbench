import asyncio
from types import SimpleNamespace
from typing import Any
from typing import cast

from pytest import MonkeyPatch

from jfbench.llm import extract_reasoning_content
from jfbench.llm import LLMClient


class _LocalStubClient(LLMClient):
    def __init__(self) -> None:
        self.provider = "local"
        self.model = "stub"
        self.extra_body: dict[str, Any] = {}
        self.client = object()
        self.use_tqdm_calls: list[bool] = []

    def _ask_local(self, prompts: list[str], use_tqdm: bool) -> tuple[list[str], list[Any]]:
        assert prompts == ["prompt"]
        self.use_tqdm_calls.append(use_tqdm)
        return ["answer"], ["detail"]


class _OpenRouterStubClient(LLMClient):
    def __init__(self) -> None:
        self.provider = "openrouter"
        self.model = "stub"
        self.extra_body: dict[str, Any] = {}
        self.client = object()
        self.use_tqdm_calls: list[bool] = []
        self.semaphore = None

    async def _ask_openai(
        self,
        prompts: list[str],
        *,
        use_tqdm: bool = False,
    ) -> tuple[list[str], list[Any]]:
        assert prompts == ["prompt"]
        self.use_tqdm_calls.append(use_tqdm)
        return ["answer"], ["detail"]


def test_ask_returns_response_details() -> None:
    client = _LocalStubClient()

    responses, details = client.ask(["prompt"])

    assert responses == ["answer"]
    assert details == ["detail"]
    assert client.use_tqdm_calls == [False]


def test_async_ask_returns_response_details_openrouter() -> None:
    client = _OpenRouterStubClient()

    responses, details = asyncio.run(client.async_ask(["prompt"]))

    assert responses == ["answer"]
    assert details == ["detail"]
    assert client.use_tqdm_calls == [False]


def test_async_ask_returns_response_details_local() -> None:
    client = _LocalStubClient()

    responses, details = asyncio.run(client.async_ask(["prompt"]))

    assert responses == ["answer"]
    assert details == ["detail"]
    assert client.use_tqdm_calls == [False]


def test_async_ask_uses_asyncio_to_thread(monkeypatch: MonkeyPatch) -> None:
    client = _LocalStubClient()
    captured: dict[str, Any] = {}

    async def _fake_to_thread(func: object, *args: object, **kwargs: object) -> object:
        captured["func"] = func
        captured["args"] = args
        captured["kwargs"] = kwargs
        return ["thread-answer"], ["thread-detail"]

    monkeypatch.setattr("jfbench.llm.asyncio.to_thread", _fake_to_thread)

    responses, details = asyncio.run(client.async_ask(["prompt"], use_tqdm=True))

    bound = cast("Any", captured["func"])
    expected = cast("Any", client._ask_local)  # noqa: SLF001
    assert getattr(bound, "__func__", None) is getattr(expected, "__func__", None)
    assert getattr(bound, "__self__", None) is client
    assert captured["args"] == (["prompt"],)
    assert captured["kwargs"] == {"use_tqdm": True}
    assert responses == ["thread-answer"]
    assert details == ["thread-detail"]


def test_ask_respects_use_tqdm_flag() -> None:
    client = _LocalStubClient()

    client.ask(["prompt"], use_tqdm=True)

    assert client.use_tqdm_calls == [True]


def test_async_ask_respects_use_tqdm_flag_openrouter() -> None:
    client = _OpenRouterStubClient()

    asyncio.run(client.async_ask(["prompt"], use_tqdm=True))

    assert client.use_tqdm_calls == [True]


def test_vllm_client_accepts_custom_base_url(monkeypatch: MonkeyPatch) -> None:
    created: dict[str, Any] = {}

    class _DummyAsyncOpenAI:
        def __init__(self, *, base_url: str, api_key: str, timeout: int | None = None) -> None:
            _ = timeout
            created["base_url"] = base_url
            created["api_key"] = api_key

    monkeypatch.setattr("jfbench.llm.AsyncOpenAI", _DummyAsyncOpenAI)

    client = LLMClient(
        provider="vllm",
        model="remote-model",
        extra_body={"base_url": "http://remote-host:9000/v1", "temperature": 0.65},
    )

    assert created["base_url"] == "http://remote-host:9000/v1"
    assert created["api_key"] == "unsed"
    assert client.temperature == 0.65
    assert client.extra_body == {}


def test_openrouter_client_enables_reasoning_by_default(monkeypatch: MonkeyPatch) -> None:
    class _DummyAsyncOpenAI:
        def __init__(self, *, base_url: str, api_key: str) -> None:
            self.base_url = base_url
            self.api_key = api_key

    monkeypatch.setenv("OPENROUTER_API_KEY", "dummy")
    monkeypatch.setattr("jfbench.llm.AsyncOpenAI", _DummyAsyncOpenAI)

    client = LLMClient(provider="openrouter", model="remote-model")

    assert client.extra_body == {}
    assert client.temperature == 0.0


def test_extract_reasoning_content_supports_vllm_detail() -> None:
    detail = SimpleNamespace(
        choices=[SimpleNamespace(message=SimpleNamespace(reasoning_content="steps"))]
    )

    reasoning = extract_reasoning_content("vllm", detail)

    assert reasoning == "steps"


def test_extract_reasoning_content_supports_openrouter_reasoning_details() -> None:
    detail = SimpleNamespace(
        choices=[
            SimpleNamespace(
                message=SimpleNamespace(
                    reasoning=None, reasoning_details=[SimpleNamespace(text="reasoned text")]
                )
            )
        ]
    )

    reasoning = extract_reasoning_content("openrouter", detail)

    assert reasoning == "reasoned text"
